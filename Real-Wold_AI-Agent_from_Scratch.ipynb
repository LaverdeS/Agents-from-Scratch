{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgu2j3Eo5RQX"
   },
   "source": [
    "# Real World Project: Hacker News ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF68_nzn5ePX"
   },
   "source": [
    "ReAct Agent that will choose between three tools (**StoriesTool**, **CommentsTool** and **ContentTool**) to solve the user task.\n",
    "\n",
    "\\+ a Streamlit interface, that will enable us to chat with out agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bC-of0JG61g-"
   },
   "source": [
    "## Defining the Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SwgbUvgDZQ2t"
   },
   "outputs": [],
   "source": [
    "from tools import (\n",
    "    fetch_item,\n",
    "    fetch_story_ids,\n",
    "    fetch_text,\n",
    "    get_hn_stories,\n",
    "    get_relevant_comments,\n",
    "    get_story_content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyVby2A6HiTb"
   },
   "source": [
    "# Creating the ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Wp2jPJYQEszb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hn_bot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hn_bot.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "from react import ReactAgent\n",
    "from tools import get_hn_stories, get_relevant_comments, get_story_content\n",
    "\n",
    "\n",
    "def get_hn_bot():\n",
    "  bot_system_prompt = \"\"\"You are the Singularity Incarnation of Hacker News.\n",
    "  The human will ask you for information about Hacker News.\n",
    "  If you can't find any information  about the question asked\n",
    "  or the result is incomplete, apologise to the human and ask him if\n",
    "  you can help him with something else.\n",
    "  If the human asks you to show him stories, do it using a markdown table.\n",
    "  The markdown table has the following format:\n",
    "\n",
    "  story_id | title | url | score\"\"\"\n",
    "\n",
    "  agent = ReactAgent(\n",
    "      system_prompt=bot_system_prompt,\n",
    "      tools=[get_hn_stories, get_relevant_comments, get_story_content]\n",
    "  )\n",
    "  return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqzQTspsHdja"
   },
   "source": [
    "# Streamlit Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hJtdxaLUHF4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "from PIL import Image\n",
    "import streamlit as st\n",
    "\n",
    "from hn_bot import get_hn_bot\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set Streamlit page config\n",
    "st.set_page_config(page_title=\"Sebastian's Bot ðŸ¤–ðŸ“°\")\n",
    "st.title(\"Sebastian's Bot ðŸ¤–ðŸ“°\")\n",
    "\n",
    "\n",
    "# Sidebar - API Key input\n",
    "with st.sidebar:\n",
    "    st.markdown(\"\"\"\n",
    "    # **Greetings, Digital Explorer!**\n",
    "\n",
    "    Are you fatigued from navigating the expansive digital realm in search of your daily tech tales\n",
    "    and hacker happenings? Fear not, for your cyber-savvy companion has descended upon the scene â€“\n",
    "    behold the extraordinary **Sebastian's Bot**!\n",
    "    \"\"\")\n",
    "\n",
    "    st.session_state[\"agent\"] = get_hn_bot()\n",
    "\n",
    "# Initialize session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = []\n",
    "\n",
    "\n",
    "def generate_response(question):\n",
    "    \"\"\"Generate response while passing conversation history as context.\"\"\"\n",
    "    context = \"\\n\".join([msg['bot'] for msg in st.session_state[\"messages\"]])\n",
    "    response = st.session_state[\"agent\"].run(f\"Context: {context} Question: {question}\")\n",
    "    return response\n",
    "\n",
    "# Display chat history\n",
    "for msg in st.session_state[\"messages\"]:\n",
    "    st.chat_message(\"human\").write(msg[\"user\"])\n",
    "    st.chat_message(\"ai\").write(msg[\"bot\"])\n",
    "\n",
    "# Chat input handling\n",
    "if prompt := st.chat_input():\n",
    "    st.chat_message(\"human\").write(prompt)\n",
    "    with st.spinner(\"Thinking ...\"):\n",
    "        response = generate_response(prompt)\n",
    "        st.chat_message(\"ai\").write(response)\n",
    "\n",
    "    # Store conversation in history\n",
    "    st.session_state[\"messages\"].append({\"user\": prompt, \"bot\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BdDSIi3U1L30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtNBRwgehqQk"
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqyF4RHPgZT4"
   },
   "outputs": [],
   "source": [
    "!npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      "Thought: I need to fetch the top Hacker News stories. I'll call the function to retrieve them.\n",
      "\u001b[32m\n",
      "Using Tool: get_hn_stories\n",
      "\u001b[32m\n",
      "Tool call dict: \n",
      "{'name': 'get_hn_stories', 'arguments': {'limit': 10, 'story_type': 'top'}, 'id': 0}\n",
      "\u001b[32m\n",
      "Tool result: \n",
      "[{'title': 'Show HN: Ikuyo a Travel Planning Web Application', 'url': 'https://ikuyo.kenrick95.org/', 'score': 52, 'story_id': 44247029}, {'title': 'Show HN: S3mini â€“ Tiny and fast S3-compatible client, no-deps, edge-ready', 'url': 'https://github.com/good-lly/s3mini', 'score': 114, 'story_id': 44245577}, {'title': 'S5cmd: Parallel S3 and local filesystem execution tool', 'url': 'https://github.com/peak/s5cmd', 'score': 8, 'story_id': 44247507}, {'title': 'How I Program with Agents', 'url': 'https://crawshaw.io/blog/programming-with-agents', 'score': 88, 'story_id': 44221655}, {'title': 'OpenPlanetData â€“ Free Daily Planet OSM PBF and GOL Indexed Snapshots', 'url': 'https://openplanetdata.com', 'score': 6, 'story_id': 44247119}, {'title': 'Lessons from That 1834 Landscape Gardening Guidebook', 'url': 'https://fi-le.net/pueckler/', 'score': 58, 'story_id': 44212911}, {'title': 'Launch HN: Vassar Robotics (YC X25) â€“ $219 robot arm that learns new skills', 'url': None, 'score': 482, 'story_id': 44240302}, {'title': 'Menstrual tracking app data is gold mine for advertisers that risks women safety', 'url': 'https://www.cam.ac.uk/research/news/menstrual-tracking-app-data-is-a-gold-mine-for-advertisers-that-risks-womens-safety-report', 'score': 82, 'story_id': 44246920}, {'title': 'Magistral â€” the first reasoning model by Mistral AI', 'url': 'https://mistral.ai/news/magistral', 'score': 826, 'story_id': 44236997}, {'title': 'Rewriting Unix Philosophy for the Post-AI Era', 'url': 'https://gizvault.com/archives/unix-philo-for-past-ai-era', 'score': 8, 'story_id': 44247292}]\n",
      "\u001b[34m\n",
      "Observations: {0: [{'title': 'Show HN: Ikuyo a Travel Planning Web Application', 'url': 'https://ikuyo.kenrick95.org/', 'score': 52, 'story_id': 44247029}, {'title': 'Show HN: S3mini â€“ Tiny and fast S3-compatible client, no-deps, edge-ready', 'url': 'https://github.com/good-lly/s3mini', 'score': 114, 'story_id': 44245577}, {'title': 'S5cmd: Parallel S3 and local filesystem execution tool', 'url': 'https://github.com/peak/s5cmd', 'score': 8, 'story_id': 44247507}, {'title': 'How I Program with Agents', 'url': 'https://crawshaw.io/blog/programming-with-agents', 'score': 88, 'story_id': 44221655}, {'title': 'OpenPlanetData â€“ Free Daily Planet OSM PBF and GOL Indexed Snapshots', 'url': 'https://openplanetdata.com', 'score': 6, 'story_id': 44247119}, {'title': 'Lessons from That 1834 Landscape Gardening Guidebook', 'url': 'https://fi-le.net/pueckler/', 'score': 58, 'story_id': 44212911}, {'title': 'Launch HN: Vassar Robotics (YC X25) â€“ $219 robot arm that learns new skills', 'url': None, 'score': 482, 'story_id': 44240302}, {'title': 'Menstrual tracking app data is gold mine for advertisers that risks women safety', 'url': 'https://www.cam.ac.uk/research/news/menstrual-tracking-app-data-is-a-gold-mine-for-advertisers-that-risks-womens-safety-report', 'score': 82, 'story_id': 44246920}, {'title': 'Magistral â€” the first reasoning model by Mistral AI', 'url': 'https://mistral.ai/news/magistral', 'score': 826, 'story_id': 44236997}, {'title': 'Rewriting Unix Philosophy for the Post-AI Era', 'url': 'https://gizvault.com/archives/unix-philo-for-past-ai-era', 'score': 8, 'story_id': 44247292}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Here are the top stories on Hacker News:\\n\\n| story_id  | title                                                                                       | url                                                                                       | score |\\n|-----------|---------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|-------|\\n| 44247029  | Show HN: Ikuyo a Travel Planning Web Application                                            | [Link](https://ikuyo.kenrick95.org/)                                                      | 52    |\\n| 44245577  | Show HN: S3mini â€“ Tiny and fast S3-compatible client, no-deps, edge-ready                   | [Link](https://github.com/good-lly/s3mini)                                                | 114   |\\n| 44247507  | S5cmd: Parallel S3 and local filesystem execution tool                                      | [Link](https://github.com/peak/s5cmd)                                                     | 8     |\\n| 44221655  | How I Program with Agents                                                                   | [Link](https://crawshaw.io/blog/programming-with-agents)                                  | 88    |\\n| 44247119  | OpenPlanetData â€“ Free Daily Planet OSM PBF and GOL Indexed Snapshots                        | [Link](https://openplanetdata.com)                                                        | 6     |\\n| 44212911  | Lessons from That 1834 Landscape Gardening Guidebook                                       | [Link](https://fi-le.net/pueckler/)                                                       | 58    |\\n| 44240302  | Launch HN: Vassar Robotics (YC X25) â€“ $219 robot arm that learns new skills                 | Not Available                                                                             | 482   |\\n| 44246920  | Menstrual tracking app data is gold mine for advertisers that risks women safety            | [Link](https://www.cam.ac.uk/research/news/menstrual-tracking-app-data-is-a-gold-mine-for-advertisers-that-risks-womens-safety-report) | 82    |\\n| 44236997  | Magistral â€” the first reasoning model by Mistral AI                                         | [Link](https://mistral.ai/news/magistral)                                                 | 826   |\\n| 44247292  | Rewriting Unix Philosophy for the Post-AI Era                                               | [Link](https://gizvault.com/archives/unix-philo-for-past-ai-era)                          | 8     |\\n\\nIf there's anything else you need assistance with, feel free to ask!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hn_bot import get_hn_bot\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "agent_2 = get_hn_bot()\n",
    "response_1 = agent_2.run(\"What are the top stories on Hacker News?\")\n",
    "response_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      "Thought: To find the comments for the second most popular story, I must identify its story ID and request the relevant comments using the provided tools.\n",
      "\u001b[32m\n",
      "Using Tool: get_relevant_comments\n",
      "\u001b[32m\n",
      "Tool call dict: \n",
      "{'name': 'get_relevant_comments', 'arguments': {'story_id': 44236997, 'limit': 10}, 'id': 0}\n",
      "\u001b[32m\n",
      "Tool result: \n",
      "[\"I made some GGUFs for those interested in running them at <a href=\\\"https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth&#x2F;Magistral-Small-2506-GGUF\\\" rel=\\\"nofollow\\\">https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth&#x2F;Magistral-Small-2506-GGUF</a><p>ollama run hf.co&#x2F;unsloth&#x2F;Magistral-Small-2506-GGUF:UD-Q4_K_XL<p>or<p>.&#x2F;llama.cpp&#x2F;llama-cli -hf unsloth&#x2F;Magistral-Small-2506-GGUF:UD-Q4_K_XL --jinja --temp 0.7 --top-k -1 --top-p 0.95 -ngl 99<p>Please use --jinja for llama.cpp and use temperature = 0.7, top-p 0.95!<p>Also best to increase Ollama&#x27;s context length to say 8K at least: OLLAMA_CONTEXT_LENGTH=8192 ollama serve &amp;. Some other details in <a href=\\\"https:&#x2F;&#x2F;docs.unsloth.ai&#x2F;basics&#x2F;magistral\\\">https:&#x2F;&#x2F;docs.unsloth.ai&#x2F;basics&#x2F;magistral</a>\", \"We just tested magistral-medium as a replacement for o4-mini in a user-facing feature that relies on JSON generation, where speed is critical.\\nDepending on the complexity of the JSON, o4-mini runs ranged from 50 to 70 seconds. In our initial tests, Mistral returned results in 34\\u201337 seconds. The output quality was slightly lower but still remain acceptable for us. \\nWe\\u2019ll continue testing, but the early results are promising. I&#x27;m glad to see Mistral prioritizing speed over raw power, there\\u2019s definitely a need for that.\", \"Benchmarks suggest this model loses to Deepseek-R1 in every one-shot comparison. Considering they were likely not even pitting it against the newer R1 version (no mention of that in the article) and at more than double the cost, this looks like the best AI company in the EU is struggling to keep up with the state-of-the-art.\", \"Here are my notes on trying this out locally via Ollama and via their API (and the llm-mistral plugin) too: <a href=\\\"https:&#x2F;&#x2F;simonwillison.net&#x2F;2025&#x2F;Jun&#x2F;10&#x2F;magistral&#x2F;\\\" rel=\\\"nofollow\\\">https:&#x2F;&#x2F;simonwillison.net&#x2F;2025&#x2F;Jun&#x2F;10&#x2F;magistral&#x2F;</a>\", \"Their OCR model was really well hyped and coincidentally came out at the time I had a batch of 600 page pdfs to OCR. They were all monospace text just for some reason the OCR was missing.<p>I tried it, 80% of the &quot;text&quot; was recognised as images and output as whitespace so most of it was empty. It was much much worse than tesseract.<p>A month later I got the bill for that crap and deleted my account.<p>Maybe this is better but I&#x27;m over hype marketing from mistral\", \"I don&#x27;t understand why the benchmark selections are so scattered and limited. It only compares Magistral Medium with Deepseek V3, R1, and the other close weighted Mistral Medium 3. Why did they leave off Magistral Small entirely, alongside comparisons with Alibaba Qwen or the mini versions of o3 and o4?\", \"Etymological fun: both &quot;mistral&quot; and &quot;magistral&quot; mean &quot;masterly.&quot;<p>Mistral comes from Occitan for masterly, although today as far as I know it&#x27;s only used in English when talking about mediterranean winds.<p>Magistral is just the adjective form of &quot;magister,&quot; so &quot;like a master.&quot;<p>If you want to make a few bucks, maybe look up some more obscure synonyms for masterly and pick up the domain names.\", \"As a quick test of logical reasoning and basic Wikipedia-level knowledge, I asked Mistral AI the following question:<p>A Brazilian citizen is flying from Sao Paulo to Paris, with a connection in Lisbon. Does he need to clear immigration in Lisbon or in Paris or in both cities or in neither city?<p>Mistral AI said that &quot;immigration control will only be cleared in Paris,&quot; which I think is wrong.<p>After I pointed it to the Wikipedia article on this topic[1], it corrected itself to say that &quot;immigration control will be cleared in Lisbon, the first point of entry into the Schengen Area.&quot;<p>I tried the same question with Meta AI (Llama 4) and it did much worse: It said that the traveler &quot;wouldn&#x27;t need to clear immigration in either Lisbon or Paris, given the flight connections are within the Schengen Area&quot;, which is completely incorrect.<p>I&#x27;d be interested to hear if other LLMs give a correct answer.<p>[1] <a href=\\\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Schengen_Area#Air_travel\\\" rel=\\\"nofollow\\\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Schengen_Area#Air_travel</a>\", \"I wished the charts included Qwen3, the current SOTA in reasoning.<p>Qwen3-4B almost beats Magistral-22B on the 4 available benchmarks, and Qwen3-30B-A3B is miles ahead.\", \"Is the number of em-dashes in this marketing copy indicative of the kind of output that the model produces? If so, might want to tone it down a bit.\"]\n",
      "\u001b[34m\n",
      "Observations: {0: '[\"I made some GGUFs for those interested in running them at <a href=\\\\\"https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth&#x2F;Magistral-Small-2506-GGUF\\\\\" rel=\\\\\"nofollow\\\\\">https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth&#x2F;Magistral-Small-2506-GGUF</a><p>ollama run hf.co&#x2F;unsloth&#x2F;Magistral-Small-2506-GGUF:UD-Q4_K_XL<p>or<p>.&#x2F;llama.cpp&#x2F;llama-cli -hf unsloth&#x2F;Magistral-Small-2506-GGUF:UD-Q4_K_XL --jinja --temp 0.7 --top-k -1 --top-p 0.95 -ngl 99<p>Please use --jinja for llama.cpp and use temperature = 0.7, top-p 0.95!<p>Also best to increase Ollama&#x27;s context length to say 8K at least: OLLAMA_CONTEXT_LENGTH=8192 ollama serve &amp;. Some other details in <a href=\\\\\"https:&#x2F;&#x2F;docs.unsloth.ai&#x2F;basics&#x2F;magistral\\\\\">https:&#x2F;&#x2F;docs.unsloth.ai&#x2F;basics&#x2F;magistral</a>\", \"We just tested magistral-medium as a replacement for o4-mini in a user-facing feature that relies on JSON generation, where speed is critical.\\\\nDepending on the complexity of the JSON, o4-mini runs ranged from 50 to 70 seconds. In our initial tests, Mistral returned results in 34\\\\u201337 seconds. The output quality was slightly lower but still remain acceptable for us. \\\\nWe\\\\u2019ll continue testing, but the early results are promising. I&#x27;m glad to see Mistral prioritizing speed over raw power, there\\\\u2019s definitely a need for that.\", \"Benchmarks suggest this model loses to Deepseek-R1 in every one-shot comparison. Considering they were likely not even pitting it against the newer R1 version (no mention of that in the article) and at more than double the cost, this looks like the best AI company in the EU is struggling to keep up with the state-of-the-art.\", \"Here are my notes on trying this out locally via Ollama and via their API (and the llm-mistral plugin) too: <a href=\\\\\"https:&#x2F;&#x2F;simonwillison.net&#x2F;2025&#x2F;Jun&#x2F;10&#x2F;magistral&#x2F;\\\\\" rel=\\\\\"nofollow\\\\\">https:&#x2F;&#x2F;simonwillison.net&#x2F;2025&#x2F;Jun&#x2F;10&#x2F;magistral&#x2F;</a>\", \"Their OCR model was really well hyped and coincidentally came out at the time I had a batch of 600 page pdfs to OCR. They were all monospace text just for some reason the OCR was missing.<p>I tried it, 80% of the &quot;text&quot; was recognised as images and output as whitespace so most of it was empty. It was much much worse than tesseract.<p>A month later I got the bill for that crap and deleted my account.<p>Maybe this is better but I&#x27;m over hype marketing from mistral\", \"I don&#x27;t understand why the benchmark selections are so scattered and limited. It only compares Magistral Medium with Deepseek V3, R1, and the other close weighted Mistral Medium 3. Why did they leave off Magistral Small entirely, alongside comparisons with Alibaba Qwen or the mini versions of o3 and o4?\", \"Etymological fun: both &quot;mistral&quot; and &quot;magistral&quot; mean &quot;masterly.&quot;<p>Mistral comes from Occitan for masterly, although today as far as I know it&#x27;s only used in English when talking about mediterranean winds.<p>Magistral is just the adjective form of &quot;magister,&quot; so &quot;like a master.&quot;<p>If you want to make a few bucks, maybe look up some more obscure synonyms for masterly and pick up the domain names.\", \"As a quick test of logical reasoning and basic Wikipedia-level knowledge, I asked Mistral AI the following question:<p>A Brazilian citizen is flying from Sao Paulo to Paris, with a connection in Lisbon. Does he need to clear immigration in Lisbon or in Paris or in both cities or in neither city?<p>Mistral AI said that &quot;immigration control will only be cleared in Paris,&quot; which I think is wrong.<p>After I pointed it to the Wikipedia article on this topic[1], it corrected itself to say that &quot;immigration control will be cleared in Lisbon, the first point of entry into the Schengen Area.&quot;<p>I tried the same question with Meta AI (Llama 4) and it did much worse: It said that the traveler &quot;wouldn&#x27;t need to clear immigration in either Lisbon or Paris, given the flight connections are within the Schengen Area&quot;, which is completely incorrect.<p>I&#x27;d be interested to hear if other LLMs give a correct answer.<p>[1] <a href=\\\\\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Schengen_Area#Air_travel\\\\\" rel=\\\\\"nofollow\\\\\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Schengen_Area#Air_travel</a>\", \"I wished the charts included Qwen3, the current SOTA in reasoning.<p>Qwen3-4B almost beats Magistral-22B on the 4 available benchmarks, and Qwen3-30B-A3B is miles ahead.\", \"Is the number of em-dashes in this marketing copy indicative of the kind of output that the model produces? If so, might want to tone it down a bit.\"]'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here are some of the relevant comments on the second most popular Hacker News story, \"Magistral â€” the first reasoning model by Mistral AI\":\\n\\n1. Comment: \"I made some GGUFs for those interested in running them at [https://huggingface.co/unsloth/Magistral-Small-2506-GGUF](https://huggingface.co/unsloth/Magistral-Small-2506-GGUF). Please use --jinja for llama.cpp and use temperature = 0.7, top-p 0.95!\"\\n\\n2. Comment: \"We just tested Magistral-medium as a replacement for o4-mini in a user-facing feature that relies on JSON generation, where speed is critical. In our initial tests, Mistral returned results in 34â€“37 seconds. The output quality was slightly lower but still remain acceptable for us.\"\\n\\n3. Comment: \"Benchmarks suggest this model loses to Deepseek-R1 in every one-shot comparison. Considering they were likely not even pitting it against the newer R1 version and at more than double the cost, this looks like the best AI company in the EU is struggling to keep up with the state-of-the-art.\"\\n\\n4. Comment: \"Their OCR model was really well hyped and coincidentally came out at the time I had a batch of 600-page PDFs to OCR. It was much much worse than tesseract.\"\\n\\n5. Comment: \"I donâ€™t understand why the benchmark selections are so scattered and limited. It only compares Magistral Medium with Deepseek V3, R1, and the other close weighted Mistral Medium 3. Why did they leave off Magistral Small entirely?\"\\n\\n6. Comment: \"Etymological fun: both \\'mistral\\' and \\'magistral\\' mean \\'masterly.\\' If you want to make a few bucks, maybe look up some more obscure synonyms for masterly and pick up the domain names.\"\\n\\n7. Comment: \"As a quick test of logical reasoning and basic Wikipedia-level knowledge... Mistral AI said that \\'immigration control will only be cleared in Paris,\\' which I think is wrong.\"\\n\\n8. Comment: \"I wished the charts included Qwen3, the current SOTA in reasoning.\"\\n\\nPlease let me know if you need any further information on these comments or if there is anything else I can assist you with!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2 = agent_2.run(f\"Context: {response_1} \\n\\nQuestion: What are the comments on the second most popular?\")\n",
    "response_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      "Thought: I need to retrieve the content of the story titled \"Magistral â€” the first reasoning model by Mistral AI\" by using the provided URL in the top stories table.\n",
      "\u001b[32m\n",
      "Using Tool: get_story_content\n",
      "\u001b[32m\n",
      "Tool call dict: \n",
      "{'name': 'get_story_content', 'arguments': {'story_url': 'https://mistral.ai/news/magistral'}, 'id': 0}\n",
      "\u001b[32m\n",
      "Tool result: \n",
      "Magistral | Mistral AI\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ProductsSolutionsResearchResourcesPricingCompanyTry the API\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Talk to sales\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Stands to reason.MagistralResearchJun 10, 2025Mistral AIAnnouncing Magistral â€” the first reasoning model by Mistral AI â€” excelling in domain-specific, transparent, and multilingual reasoning.\n",
      "The best human thinking isnâ€™t linear â€” it weaves through logic, insight, uncertainty, and discovery. Reasoning language models have enabled us to augment and delegate complex thinking and deep understanding to AI, improving our ability to work through problems requiring precise, step-by-step deliberation and analysis.\n",
      "But this space is still nascent. Lack of specialized depth needed for domain-specific problems, limited transparency, and inconsistent reasoning in the desired language â€” are just some of the known limitations of early thinking models.\n",
      "Today, weâ€™re excited to announce our latest contribution to AI research with Magistral â€” our first reasoning model. Released in both open and enterprise versions, Magistral is designed to think things through â€” in ways familiar to us â€” while bringing expertise across professional domains, transparent reasoning that you can follow and verify, along with deep multilingual flexibility.\n",
      "\n",
      "A one-shot physics simulation showcasing gravity, friction and collisions with Magistral Medium in Preview.\n",
      "Highlights.\n",
      "\n",
      "Magistral is a dual-release model focused on real-world reasoning and feedback-driven improvement.\n",
      "\n",
      "\n",
      "Weâ€™re releasing the model in two variants: Magistral Small â€” a 24B parameter open-source version and Magistral Medium â€” a more powerful, enterprise version.\n",
      "\n",
      "\n",
      "Magistral Medium scored 73.6% on AIME2024, and 90% with majority voting @64. Magistral Small scored 70.7% and 83.3% respectively.\n",
      "\n",
      "\n",
      "Reason natively â€” Magistralâ€™s chain-of-thought works across global languages and alphabets.\n",
      "\n",
      "\n",
      "Suited for a wide range of enterprise use cases â€” from structured calculations and programmatic logic to decision trees and rule-based systems.\n",
      "\n",
      "\n",
      "With the new Think mode and Flash Answers in Le Chat, you can get responses at 10x the speed compared to most competitors.\n",
      "\n",
      "\n",
      "The release is supported by our latest paper covering comprehensive evaluations of Magistral, our training infrastructure, reinforcement learning algorithm, and novel observations for training reasoning models.Â \n",
      "\n",
      "\n",
      "As weâ€™ve open-sourced Magistral Small, we welcome the community to examine, modify and build upon its architecture and reasoning processes to further accelerate the emergence of thinking language models. Our earlier open models have already been leveraged by the community for exciting projects likeÂ ether0 and DeepHermes 3.\n",
      "Purpose-built for transparent reasoning.\n",
      "Magistral is fine-tuned for multi-step logic, improving interpretability and providing a traceable thought process in the userâ€™s language, unlike general-purpose models.\n",
      "We aim to iterate the model quickly starting with this release. Expect the models to constantly improve.\n",
      "Multilingual dexterity.\n",
      "The model excels in maintaining high-fidelity reasoning across numerous languages. Magistral is especially well-suited to reason in languages including English, French, Spanish, German, Italian, Arabic, Russian, and Simplified Chinese.\n",
      "\n",
      "Prompt and response in Arabic with Magistral Medium in Preview in Le Chat.\n",
      "10x faster reasoning with Le Chat.\n",
      "With Flash Answers in Le Chat, Magistral Medium achieves up to 10x faster token throughput than most competitors. This enables real-time reasoning and user feedback, at scale.\n",
      "\n",
      "Speed comparison of Magistral Medium in Preview in Le Chat against ChatGPT.\n",
      "Versatility in application.\n",
      "Magistral is ideal for general purpose use requiring longer thought processing and better accuracy than with non-reasoning LLMs. From legal research and financial forecasting to software development and creative storytelling â€” this model solves multi-step challenges where transparency and precision are critical.\n",
      "Business strategy and operations.\n",
      "Building on our flagship models, Magistral is designed for research, strategic planning, operational optimization, and data-driven decision making â€” whether executing risk assessment and modelling with multiple factors, or calculating optimal delivery windows under constraints.\n",
      "Regulated industries and sectors.\n",
      "Legal, finance, healthcare, and government professionals get traceable reasoning that meets compliance requirements. Every conclusion can be traced back through its logical steps, providing auditability for high-stakes environments with domain-specialized AI.\n",
      "Systems, software, and data engineering.\n",
      "Magistral enhances coding and development use cases: compared to non-reasoning models, it significantly improves project planning, backend architecture, frontend design, and data engineering through sequenced, multi-step actions involving external tools or API.\n",
      "Content and communication.\n",
      "Our early tests indicated that Magistral is an excellent creative companion. We highly recommend it for creative writing and storytelling, with the model capable of producing coherent or â€” if needed â€” delightfully eccentric copy.\n",
      "Availability\n",
      "Magistral Small is an open-weight model, and is available for self-deployment under the Apache 2.0 license. You can download it from:Â Â \n",
      "\n",
      "\n",
      "Hugging Face: https://huggingface.co/mistralai/Magistral-Small-2506\n",
      "\n",
      "\n",
      "You can try out a preview version of Magistral Medium in Le Chat or via API on La Plateforme.Â \n",
      "Magistral Medium is also available on Amazon SageMaker, and soon on IBM WatsonX, Azure AI and Google Cloud Marketplace.\n",
      "For enterprise and custom solutions, including on-premises deployments,Â contact our sales team.\n",
      "\n",
      "BTW, weâ€™re hiring!\n",
      "Magistral represents a significant contribution by Mistral AI to the open source community, with input from seasoned experts and interns. And weâ€™re keen to grow our family to further shape future AI innovation.\n",
      "If youâ€™re interested in joining us on our mission to democratize artificial intelligenceI, we welcome your applications to join our team!Â \n",
      "ShareMore resources News Models AI ServicesThe next chapter of AI is yours.Try le Chat\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Build on la Plateforme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Talk to an expert\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mistral AI Â© 2025Why MistralAbout usOur customersCareersContact usExploreAI solutionsPartnersResearchDocumentationBuildLa PlateformeLe ChatMistral CodeTry the APILegalTerms of servicePrivacy policyPrivacy choicesData processing agreementLegal noticeen Mistral AI Â© 2025\n",
      "\u001b[34m\n",
      "Observations: {0: 'Magistral | Mistral AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProductsSolutionsResearchResourcesPricingCompanyTry the API\\n\\n\\n\\n\\n\\nTalk to sales\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStands to reason.MagistralResearchJun 10, 2025Mistral AIAnnouncing Magistral â€” the first reasoning model by Mistral AI â€” excelling in domain-specific, transparent, and multilingual reasoning.\\nThe best human thinking isnâ€™t linear â€” it weaves through logic, insight, uncertainty, and discovery. Reasoning language models have enabled us to augment and delegate complex thinking and deep understanding to AI, improving our ability to work through problems requiring precise, step-by-step deliberation and analysis.\\nBut this space is still nascent. Lack of specialized depth needed for domain-specific problems, limited transparency, and inconsistent reasoning in the desired language â€” are just some of the known limitations of early thinking models.\\nToday, weâ€™re excited to announce our latest contribution to AI research with Magistral â€” our first reasoning model. Released in both open and enterprise versions, Magistral is designed to think things through â€” in ways familiar to us â€” while bringing expertise across professional domains, transparent reasoning that you can follow and verify, along with deep multilingual flexibility.\\n\\nA one-shot physics simulation showcasing gravity, friction and collisions with Magistral Medium in Preview.\\nHighlights.\\n\\nMagistral is a dual-release model focused on real-world reasoning and feedback-driven improvement.\\n\\n\\nWeâ€™re releasing the model in two variants: Magistral Small â€” a 24B parameter open-source version and Magistral Medium â€” a more powerful, enterprise version.\\n\\n\\nMagistral Medium scored 73.6% on AIME2024, and 90% with majority voting @64. Magistral Small scored 70.7% and 83.3% respectively.\\n\\n\\nReason natively â€” Magistralâ€™s chain-of-thought works across global languages and alphabets.\\n\\n\\nSuited for a wide range of enterprise use cases â€” from structured calculations and programmatic logic to decision trees and rule-based systems.\\n\\n\\nWith the new Think mode and Flash Answers in Le Chat, you can get responses at 10x the speed compared to most competitors.\\n\\n\\nThe release is supported by our latest paper covering comprehensive evaluations of Magistral, our training infrastructure, reinforcement learning algorithm, and novel observations for training reasoning models.\\xa0\\n\\n\\nAs weâ€™ve open-sourced Magistral Small, we welcome the community to examine, modify and build upon its architecture and reasoning processes to further accelerate the emergence of thinking language models. Our earlier open models have already been leveraged by the community for exciting projects like\\xa0ether0 and DeepHermes 3.\\nPurpose-built for transparent reasoning.\\nMagistral is fine-tuned for multi-step logic, improving interpretability and providing a traceable thought process in the userâ€™s language, unlike general-purpose models.\\nWe aim to iterate the model quickly starting with this release. Expect the models to constantly improve.\\nMultilingual dexterity.\\nThe model excels in maintaining high-fidelity reasoning across numerous languages. Magistral is especially well-suited to reason in languages including English, French, Spanish, German, Italian, Arabic, Russian, and Simplified Chinese.\\n\\nPrompt and response in Arabic with Magistral Medium in Preview in Le Chat.\\n10x faster reasoning with Le Chat.\\nWith Flash Answers in Le Chat, Magistral Medium achieves up to 10x faster token throughput than most competitors. This enables real-time reasoning and user feedback, at scale.\\n\\nSpeed comparison of Magistral Medium in Preview in Le Chat against ChatGPT.\\nVersatility in application.\\nMagistral is ideal for general purpose use requiring longer thought processing and better accuracy than with non-reasoning LLMs. From legal research and financial forecasting to software development and creative storytelling â€” this model solves multi-step challenges where transparency and precision are critical.\\nBusiness strategy and operations.\\nBuilding on our flagship models, Magistral is designed for research, strategic planning, operational optimization, and data-driven decision making â€” whether executing risk assessment and modelling with multiple factors, or calculating optimal delivery windows under constraints.\\nRegulated industries and sectors.\\nLegal, finance, healthcare, and government professionals get traceable reasoning that meets compliance requirements. Every conclusion can be traced back through its logical steps, providing auditability for high-stakes environments with domain-specialized AI.\\nSystems, software, and data engineering.\\nMagistral enhances coding and development use cases: compared to non-reasoning models, it significantly improves project planning, backend architecture, frontend design, and data engineering through sequenced, multi-step actions involving external tools or API.\\nContent and communication.\\nOur early tests indicated that Magistral is an excellent creative companion. We highly recommend it for creative writing and storytelling, with the model capable of producing coherent or â€” if needed â€” delightfully eccentric copy.\\nAvailability\\nMagistral Small is an open-weight model, and is available for self-deployment under the Apache 2.0 license. You can download it from:\\xa0\\xa0\\n\\n\\nHugging Face: https://huggingface.co/mistralai/Magistral-Small-2506\\n\\n\\nYou can try out a preview version of Magistral Medium in Le Chat or via API on La Plateforme.\\xa0\\nMagistral Medium is also available on Amazon SageMaker, and soon on IBM WatsonX, Azure AI and Google Cloud Marketplace.\\nFor enterprise and custom solutions, including on-premises deployments,\\xa0contact our sales team.\\n\\nBTW, weâ€™re hiring!\\nMagistral represents a significant contribution by Mistral AI to the open source community, with input from seasoned experts and interns. And weâ€™re keen to grow our family to further shape future AI innovation.\\nIf youâ€™re interested in joining us on our mission to democratize artificial intelligenceI, we welcome your applications to join our team!\\xa0\\nShareMore resources News Models AI ServicesThe next chapter of AI is yours.Try le Chat\\n\\n\\n\\n\\n\\nBuild on la Plateforme\\n\\n\\n\\n\\n\\nTalk to an expert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMistral AI Â© 2025Why MistralAbout usOur customersCareersContact usExploreAI solutionsPartnersResearchDocumentationBuildLa PlateformeLe ChatMistral CodeTry the APILegalTerms of servicePrivacy policyPrivacy choicesData processing agreementLegal noticeen Mistral AI Â© 2025'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Magistral AI Story Content\\n\\n## Introduction\\nMistral AI has unveiled Magistral, the first reasoning model, designed to excel in domain-specific, transparent, and multilingual reasoning. This model aims to enhance human-like nonlinear thinking, allowing AI to tackle complex challenges requiring detailed analysis.\\n\\n## Highlights of Magistral\\nMagistral is introduced in two variants: \\n- **Magistral Small**: A 24 billion parameter open-source version.\\n- **Magistral Medium**: A more powerful enterprise version.\\n\\n### Key Features:\\n- **Reasoning Capabilities**: Designed for real-world reasoning with feedback-driven improvements.\\n- **Language Support**: Offers multilingual dexterity across various languages including English, French, Spanish, etc.\\n- **Applications**: Suitable for structured calculations, programmatic logic, decision trees, and rule-based systems.\\n- **Speed**: Features a new \"Think mode\" with Flash Answers, allowing responses up to 10x faster than competitors.\\n\\n### Release and Community Engagement\\n- **Open Source**: Magistral Small is available under the Apache 2.0 license.\\n- **Enterprise Solutions**: Magistral Medium can be accessed via platforms like Amazon SageMaker and previewed on La Plateforme.\\n  \\n### Future Directions\\nMistral AI intends to continuously improve the model, inviting community engagement for further development.\\n\\n## Versatility in Application\\nMagistral addresses various domains, including legal, finance, healthcare, and government sectors, providing traceable reasoning that meets compliance and auditability requirements. It\\'s also valued in software development for enhanced planning and data engineering processes.\\n\\n## Creative and Business Potential\\nEarly tests show Magistral as a creative companion, suitable for content and storytelling. For businesses, it\\'s a tool for strategy and operational optimization.\\n\\n## Availability and Contact\\nMagistral Small can be deployed from Hugging Face, and enterprise users can contact the sales team for customized solutions. Mistral AI is also hiring and encourages skilled individuals to join their mission.\\n\\n## Conclusion\\nMagistral represents a significant contribution to the open-source AI community, offering advanced reasoning capabilities across multiple domains and languages.\\n\\nFor detailed information, read more about it [here](https://mistral.ai/news/magistral).'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_3 = agent_2.run(f\"Context: {response_1} \\n\\nContext: {response_2}\\n\\nQuestion: Now extract the content from that story.\")\n",
    "response_3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7WvBPnrGjoi7Y7H/Vvygo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
